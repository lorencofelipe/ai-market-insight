# PRD 01 ‚Äî RAG-Powered Market Intelligence Chat

## 1. Problema

O chat do AI Market Insight hoje usa apenas o conhecimento geral do LLM para responder perguntas de mercado. Isso causa:

- **Alucina√ß√µes**: Dados inventados sobre mercados, funding, concorrentes
- **Desatualiza√ß√£o**: Conhecimento limitado ao training cutoff do modelo
- **Sem fontes**: Usu√°rio n√£o consegue verificar a veracidade das respostas
- **Zero diferencia√ß√£o**: Qualquer ChatGPT/Gemini responde igual

> [!CAUTION]
> Sem RAG, o produto √© apenas um wrapper de LLM ‚Äî sem moat competitivo real.

---

## 2. Solu√ß√£o

Implementar **Retrieval-Augmented Generation (RAG)** para que o chat responda baseado em **dados reais de mercado** coletados pelos scripts de execu√ß√£o, reports, e fontes confi√°veis.

### Fluxo Proposto

```mermaid
graph LR
    A[Pergunta do Usu√°rio] --> B[Query Embedding]
    B --> C[Busca no Vector DB]
    C --> D[Top-K Documentos Relevantes]
    D --> E[Reranking]
    E --> F[LLM + Contexto Recuperado]
    F --> G[Resposta com Cita√ß√µes]
```

---

## 3. Escopo

### In-Scope (Must Have)
- Pipeline de ingest√£o: converter outputs dos scripts Python (`.tmp/*.json`) em embeddings
- Vector store no Supabase via `pgvector`
- Retrieval endpoint (Supabase Edge Function)
- Chat frontend com cita√ß√µes/fontes nas respostas
- Chunking inteligente por se√ß√£o de relat√≥rios

### Should Have
- Hybrid search (sem√¢ntico + keyword BM25)
- Reranking com MMR (Maximal Marginal Relevance) ‚Äî gratuito
- Indicador de confian√ßa por resposta (baseado em similarity score)
- Cache de respostas por hash da query (evita chamadas repetidas ao LLM)

### Could Have
- Upload de PDFs pelo usu√°rio (relat√≥rios de mercado)
- Scheduled re-indexing autom√°tico

### Out of Scope
- Fine-tuning de modelos de embedding
- Multi-tenancy de knowledge bases por usu√°rio (v2)

---

## 4. Arquitetura T√©cnica (3 Camadas)

### Layer 1 ‚Äî Directive
**Novo arquivo**: `directives/rag_pipeline.md`
- Define fluxo de ingest√£o, formatos aceitos, crit√©rios de chunking
- Regras de quando re-indexar dados
- Thresholds de relev√¢ncia para incluir/excluir contexto

### Layer 3 ‚Äî Execution
**Novos scripts**:

| Script | Fun√ß√£o |
|--------|--------|
| `execution/rag_ingest.py` | L√™ JSONs/textos do `.tmp/`, chunka, gera embeddings, insere no Supabase pgvector |
| `execution/rag_query.py` | Recebe query, busca documentos similares, retorna top-K com scores |

**Depend√™ncias** (adicionar ao `requirements.txt`):
```
sentence-transformers   # embeddings locais, gratuito
pgvector                # client pgvector
supabase                # j√° no stack
tiktoken                # contagem de tokens
```

> [!NOTE]
> Nenhuma nova API paga necess√°ria. Todos os componentes rodam localmente ou usam infra j√° existente (Supabase).

### Backend ‚Äî Supabase
- Extens√£o `pgvector` habilitada
- Tabela `documents`: `id`, `content`, `embedding`, `metadata`, `source`, `chunk_index`, `created_at`
- Fun√ß√£o SQL de busca por similaridade (cosine distance)
- Edge Function `rag-search` para expor via API

### Frontend
- Componente `SourceCitation.tsx` para exibir fontes inline
- Badge de confian√ßa (üü¢ Alta | üü° M√©dia | üî¥ Baixa) baseado em similarity score
- Fallback gracioso quando nenhum documento relevante √© encontrado

---

## 5. Stack Zero-Cost

| Componente | Solu√ß√£o | Custo |
|------------|---------|-------|
| **Embeddings** | `bge-small-en-v1.5` (sentence-transformers) ‚Äî local | $0 |
| **Vector Store** | Supabase pgvector ‚Äî j√° no stack | $0 |
| **Reranking** | MMR (Maximal Marginal Relevance) ‚Äî nativo LangChain | $0 |
| **Cache** | Supabase tabela `query_cache` com hash MD5 | $0 |
| **LLM** | Lovable Gateway ‚Äî j√° em uso | J√° pago |

**Modelo de embedding escolhido: `bge-small-en-v1.5`**
- Qualidade pr√≥xima ao OpenAI ada-002
- 384 dimens√µes ‚Äî leve e r√°pido
- Roda 100% local, sem API externa
- Suporta ingl√™s e portugu√™s

### Cache de Respostas
Antes de chamar o LLM, verificar se a query (ou similar) j√° foi respondida:
```python
import hashlib
query_hash = hashlib.md5(query.strip().lower().encode()).hexdigest()
# Busca no Supabase query_cache ‚Üí retorna se hit
# Salva no cache ap√≥s nova resposta gerada
```
Reduz chamadas ao LLM em ~30‚Äì40% em uso regular.

---

## 6. M√©tricas de Sucesso

| M√©trica | Target | Como Medir |
|---------|--------|------------|
| **Respostas com cita√ß√µes** | >80% das respostas | % de respostas que incluem pelo menos 1 fonte |
| **Relev√¢ncia do retrieval** | MRR >0.7 | Mean Reciprocal Rank dos documentos recuperados |
| **Redu√ß√£o de alucina√ß√µes** | -60% | Avalia√ß√£o manual de amostra (antes vs depois) |
| **Lat√™ncia** | <3s p95 | Tempo total query ‚Üí resposta com retrieval |
| **Ado√ß√£o** | >50% dos chats | % de conversas que disparam retrieval |

---

## 7. Riscos e Mitiga√ß√µes

| Risco | Severidade | Mitiga√ß√£o |
|-------|-----------|-----------|
| Base de dados vazia no in√≠cio | Alta | Seed autom√°tico com outputs dos scripts existentes |
| Lat√™ncia alta (embedding local) | M√©dia | Cache de embeddings computados, batch processing no ingest |
| Dados desatualizados | M√©dia | Metadata com timestamp, alertar quando dado >30 dias |
| Qualidade dos embeddings locais | Baixa | `bge-small-en-v1.5` tem qualidade suficiente para market intel |

---

## 8. Estimativa RICE

| | Valor |
|---|---|
| **Reach** | 100% dos usu√°rios (chat √© feature central) |
| **Impact** | 3x (Massive ‚Äî transforma a proposta de valor) |
| **Confidence** | 80% (Medium ‚Äî RAG √© padr√£o comprovado) |
| **Effort** | 3 person-weeks |
| **RICE Score** | **(100 √ó 3 √ó 0.8) / 3 = 80** |

---

## 9. Crit√©rios de Aceite

- [ ] Embeddings gerados localmente com `bge-small-en-v1.5` ‚Äî sem API paga
- [ ] Dados dos scripts `competitor_discovery.py` e `framework_analysis.py` indexados no vector store
- [ ] Cache de respostas funcional (hit retorna sem chamar o LLM)
- [ ] Chat retorna respostas com pelo menos 1 cita√ß√£o quando dados relevantes existem
- [ ] Fallback graciosa para resposta sem RAG quando similarity score < threshold
- [ ] Lat√™ncia total < 3s para 95% das queries
- [ ] Badge de confian√ßa vis√≠vel no frontend
- [ ] Custo adicional de infra: **$0**
